{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44ybhapTcoCk"
      },
      "source": [
        "구글 드라이브"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LiLCyQ0ciiO",
        "outputId": "09d6cfb1-f612-42bd-fc56-5a2cbf3de030"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbRMIXDlcrkm"
      },
      "source": [
        "json, csv 파일 경로"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4F1cHbGkc2Hm"
      },
      "outputs": [],
      "source": [
        "json_file_path1 = '/content/drive/MyDrive/Korean_chat_summary/jsonfile/개인및관계.json'  # Adjust this path\n",
        "csv_file_path1 = '/content/drive/MyDrive/Korean_chat_summary/csvfile/개인및관계.csv'    # Adjust this path\n",
        "\n",
        "json_file_path2 = '/content/drive/MyDrive/Korean_chat_summary/jsonfile/미용과건강.json'  # Adjust this path\n",
        "csv_file_path2 = '/content/drive/MyDrive/Korean_chat_summary/csvfile/미용과건강.csv'    # Adjust this path\n",
        "\n",
        "json_file_path3 = '/content/drive/MyDrive/Korean_chat_summary/jsonfile/상거래(쇼핑).json'  # Adjust this path\n",
        "csv_file_path3 = '/content/drive/MyDrive/Korean_chat_summary/csvfile/상거래(쇼핑).csv'    # Adjust this path\n",
        "\n",
        "json_file_path4 = '/content/drive/MyDrive/Korean_chat_summary/jsonfile/시사교육.json'  # Adjust this path\n",
        "csv_file_path4 = '/content/drive/MyDrive/Korean_chat_summary/csvfile/시사교육.csv'    # Adjust this path\n",
        "\n",
        "json_file_path5 = '/content/drive/MyDrive/Korean_chat_summary/jsonfile/식음료.json'  # Adjust this path\n",
        "csv_file_path5 = '/content/drive/MyDrive/Korean_chat_summary/csvfile/식음료.csv'    # Adjust this path\n",
        "\n",
        "json_file_path6 = '/content/drive/MyDrive/Korean_chat_summary/jsonfile/여가생활.json'  # Adjust this path\n",
        "csv_file_path6 = '/content/drive/MyDrive/Korean_chat_summary/csvfile/여가생활.csv'    # Adjust this path\n",
        "\n",
        "json_file_path7 = '/content/drive/MyDrive/Korean_chat_summary/jsonfile/일과직업.json'  # Adjust this path\n",
        "csv_file_path7 = '/content/drive/MyDrive/Korean_chat_summary/csvfile/일과직업.csv'    # Adjust this path\n",
        "\n",
        "json_file_path8 = '/content/drive/MyDrive/Korean_chat_summary/jsonfile/주거와생활.json'  # Adjust this path\n",
        "csv_file_path8 = '/content/drive/MyDrive/Korean_chat_summary/csvfile/주거와생활.csv'    # Adjust this path\n",
        "\n",
        "json_file_path9 = '/content/drive/MyDrive/Korean_chat_summary/jsonfile/행사.json'  # Adjust this path\n",
        "csv_file_path9 = '/content/drive/MyDrive/Korean_chat_summary/csvfile/행사.csv'    # Adjust this path\n",
        "\n",
        "\n",
        "test_json_file_path1 = '/content/drive/MyDrive/Korean_chat_summary/jsonfile_test/개인및관계.json'  # Adjust this path\n",
        "test_csv_file_path1 = '/content/drive/MyDrive/Korean_chat_summary/csvfile_test/개인및관계.csv'    # Adjust this path\n",
        "\n",
        "test_json_file_path2 = '/content/drive/MyDrive/Korean_chat_summary/jsonfile_test/미용과건강.json'  # Adjust this path\n",
        "test_csv_file_path2 = '/content/drive/MyDrive/Korean_chat_summary/csvfile_test/미용과건강.csv'    # Adjust this path\n",
        "\n",
        "test_json_file_path3 = '/content/drive/MyDrive/Korean_chat_summary/jsonfile_test/상거래(쇼핑).json'  # Adjust this path\n",
        "test_csv_file_path3 = '/content/drive/MyDrive/Korean_chat_summary/csvfile_test/상거래(쇼핑).csv'    # Adjust this path\n",
        "\n",
        "test_json_file_path4 = '/content/drive/MyDrive/Korean_chat_summary/jsonfile_test/시사교육.json'  # Adjust this path\n",
        "test_csv_file_path4 = '/content/drive/MyDrive/Korean_chat_summary/csvfile_test/시사교육.csv'    # Adjust this path\n",
        "\n",
        "test_json_file_path5 = '/content/drive/MyDrive/Korean_chat_summary/jsonfile_test/식음료.json'  # Adjust this path\n",
        "test_csv_file_path5 = '/content/drive/MyDrive/Korean_chat_summary/csvfile_test/식음료.csv'    # Adjust this path\n",
        "\n",
        "test_json_file_path6 = '/content/drive/MyDrive/Korean_chat_summary/jsonfile_test/여가생활.json'  # Adjust this path\n",
        "test_csv_file_path6 = '/content/drive/MyDrive/Korean_chat_summary/csvfile_test/여가생활.csv'    # Adjust this path\n",
        "\n",
        "test_json_file_path7 = '/content/drive/MyDrive/Korean_chat_summary/jsonfile_test/일과직업.json'  # Adjust this path\n",
        "test_csv_file_path7 = '/content/drive/MyDrive/Korean_chat_summary/csvfile_test/일과직업.csv'    # Adjust this path\n",
        "\n",
        "test_json_file_path8 = '/content/drive/MyDrive/Korean_chat_summary/jsonfile_test/주거와생활.json'  # Adjust this path\n",
        "test_csv_file_path8 = '/content/drive/MyDrive/Korean_chat_summary/csvfile_test/주거와생활.csv'    # Adjust this path\n",
        "\n",
        "test_json_file_path9 = '/content/drive/MyDrive/Korean_chat_summary/jsonfile_test/행사.json'  # Adjust this path\n",
        "test_csv_file_path9 = '/content/drive/MyDrive/Korean_chat_summary/csvfile_test/행사.csv'    # Adjust this path\n",
        "\n",
        "combined_csv_file_path = '/content/drive/MyDrive/Korean_chat_summary/csvfile/combined_dataset.csv'\n",
        "test_combined_csv_file_path = '//content/drive/MyDrive/Korean_chat_summary/csvfile_test/combined_test_dataset.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9bs3wZ9d46U"
      },
      "source": [
        "필요한 라이브러리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeSiWe3Pd6Oj",
        "outputId": "581d834e-213c-4d8f-81a4-2b88356391de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers) (0.24.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.8.30)\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.4)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (24.1)\n",
            "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.6/488.6 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.5.0 konlpy-0.6.0\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.13.2-py3-none-any.whl.metadata (5.8 kB)\n",
            "Downloading emoji-2.13.2-py3-none-any.whl (553 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m553.2/553.2 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.13.2\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install tokenizers\n",
        "!pip install konlpy\n",
        "!pip install sentencepiece\n",
        "!pip install emoji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwjJQhWCeVo-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from transformers import PegasusTokenizerFast\n",
        "from tokenizers import pre_tokenizers\n",
        "from tokenizers.implementations.sentencepiece_unigram import SentencePieceUnigramTokenizer\n",
        "from tokenizers import decoders, Regex, normalizers, pre_tokenizers, processors\n",
        "import konlpy\n",
        "from konlpy.tag import Okt\n",
        "from konlpy.tag import Mecab\n",
        "import json\n",
        "import sentencepiece as spm\n",
        "import re\n",
        "import emoji\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, PegasusTokenizerFast\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBeSCIU7eZsQ"
      },
      "source": [
        "Okt 토큰화: 한 번 실행하고 나면 그 이후로 안 해도 됨."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ByKpGFXh2Ei",
        "outputId": "17b5fc14-b647-4676-b76e-a76cdece1459"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Combining text data is completed.\n",
            "Combining and cleaning text data is completed.\n"
          ]
        }
      ],
      "source": [
        "# File paths for the individual JSON files\n",
        "file_paths = [\n",
        "    \"/content/drive/MyDrive/Korean_chat_summary/jsonfile/개인및관계.json\",\n",
        "    \"/content/drive/MyDrive/Korean_chat_summary/jsonfile/미용과건강.json\",\n",
        "    \"/content/drive/MyDrive/Korean_chat_summary/jsonfile/상거래(쇼핑).json\",\n",
        "    \"/content/drive/MyDrive/Korean_chat_summary/jsonfile/시사교육.json\",\n",
        "    \"/content/drive/MyDrive/Korean_chat_summary/jsonfile/식음료.json\",\n",
        "    \"/content/drive/MyDrive/Korean_chat_summary/jsonfile/여가생활.json\",\n",
        "    \"/content/drive/MyDrive/Korean_chat_summary/jsonfile/일과직업.json\",\n",
        "    \"/content/drive/MyDrive/Korean_chat_summary/jsonfile/주거와생활.json\",\n",
        "    \"/content/drive/MyDrive/Korean_chat_summary/jsonfile/행사.json\"\n",
        "]\n",
        "\n",
        "# Initialize an empty list to hold all the data\n",
        "combined_data = []\n",
        "\n",
        "# Collect all text data\n",
        "for file_path in file_paths:\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        data = json.load(file)\n",
        "        for conversation in data.get('data', []):\n",
        "            # Extract and append all utterances\n",
        "            for dialogue in conversation['body']['dialogue']:\n",
        "                utterance = dialogue.get('utterance', '')\n",
        "                combined_data.append(utterance)\n",
        "\n",
        "            # Extract and append the summary\n",
        "            summary = conversation['body'].get('summary', '')\n",
        "            combined_data.append(summary)\n",
        "\n",
        "print(\"Combining text data is completed.\")\n",
        "\n",
        "# Function to remove both text-based emoticons and emojis\n",
        "def remove_emoticons_and_emojis(text):\n",
        "    # Remove text-based emoticons using regex\n",
        "    text = re.sub(r':\\)|:\\(|;\\)|;\\(|:D|:P|;D|:\\||:\\*|<3|:o|:O|:/|:-/|:\\'|>:\\(|@\\)|:@|;@\\)|:@|\\^_\\^|\\._\\.', '', text)\n",
        "    # Remove emojis using the emoji library\n",
        "    text = emoji.replace_emoji(text, replace='')\n",
        "    return text\n",
        "\n",
        "# Initialize an empty list to hold all the data\n",
        "combined_data = []\n",
        "\n",
        "# Collect all text data\n",
        "for file_path in file_paths:\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        data = json.load(file)\n",
        "        for conversation in data.get('data', []):\n",
        "            # Extract and append all utterances\n",
        "            for dialogue in conversation['body']['dialogue']:\n",
        "                utterance = dialogue.get('utterance', '')\n",
        "                # Remove emoticons and emojis\n",
        "                cleaned_utterance = remove_emoticons_and_emojis(utterance)\n",
        "                combined_data.append(cleaned_utterance)\n",
        "\n",
        "            # Extract and append the summary\n",
        "            summary = conversation['body'].get('summary', '')\n",
        "            # Remove emoticons and emojis\n",
        "            cleaned_summary = remove_emoticons_and_emojis(summary)\n",
        "            combined_data.append(cleaned_summary)\n",
        "\n",
        "print(\"Combining and cleaning text data is completed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bqq0AQ5zebmg",
        "outputId": "945285ca-82ec-4997-a9c8-2141264242d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Okt tokenization is completed.\n"
          ]
        }
      ],
      "source": [
        "# Tokenize using Okt and write to file\n",
        "\n",
        "okt = Okt()\n",
        "# Save the tokenized output\n",
        "with open('/content/drive/MyDrive/Korean_chat_summary/PEGASUS_okt_tokens.txt', 'w', encoding='utf-8') as file:  # Make sure to use the full path\n",
        "    for text in combined_data:\n",
        "        tokens = okt.morphs(text)  # Tokenize with Okt\n",
        "        file.write(' '.join(tokens) + '\\n')\n",
        "\n",
        "print(\"Okt tokenization is completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vAKh2r6GSZH",
        "outputId": "4210bd94-4518-46a2-d1d1-dac9a3fb6aac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libmecab-dev is already the newest version (0.996-14build9).\n",
            "mecab-ipadic-utf8 is already the newest version (2.7.0-20070801+main-3).\n",
            "mecab is already the newest version (0.996-14build9).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Requirement already satisfied: mecab-python3 in /usr/local/lib/python3.10/dist-packages (1.0.9)\n"
          ]
        }
      ],
      "source": [
        "# Install MeCab and related packages\n",
        "!apt-get install -y mecab libmecab-dev mecab-ipadic-utf8\n",
        "!pip install mecab-python3\n",
        "\n",
        "# Create a default mecabrc file in the appropriate directory\n",
        "!echo \"dicdir = /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-utf8\" > /usr/local/etc/mecabrc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypUdaGW1HGgR",
        "outputId": "b4551261-d997-4e0e-bd60-5ac8293999f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ls: cannot access '/usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-utf8': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!ls /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-utf8\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LvyLADceqso"
      },
      "source": [
        "PEGASUS용 토크나이저 형태 설정 / SentencePiece 알고리즘"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7PrQW8GeqC6",
        "outputId": "f5aabb03-6bc3-4b73-eaf4-5504305ff20d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SentencePiece model training is completed. Model prefix: /content/drive/MyDrive/Korean_chat_summary/PEGASUS_tokenizer2/korean_bpe_unigram\n",
            "Tokenizer model files saved in the specified location.\n"
          ]
        }
      ],
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "# Function to train the SentencePiece model\n",
        "def train_sentencepiece_model(input_file, model_prefix, vocab_size):\n",
        "    # Train SentencePiece model\n",
        "    spm.SentencePieceTrainer.train(\n",
        "        f'--input={input_file} '\n",
        "        f'--model_prefix={model_prefix} '\n",
        "        f'--vocab_size={vocab_size} '\n",
        "        f'--model_type=unigram '\n",
        "        f'--unk_piece=\"<unk>\"'  # Set unknown token\n",
        "    )\n",
        "    print(f\"SentencePiece model training is completed. Model prefix: {model_prefix}\")\n",
        "\n",
        "def load_sentencepiece_model(model_file):\n",
        "    # Load the trained SentencePiece model\n",
        "    sp = spm.SentencePieceProcessor()\n",
        "    sp.load(model_file)\n",
        "    return sp\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify the input file and the model saving location\n",
        "    input_file = \"/content/drive/MyDrive/Korean_chat_summary/PEGASUS_okt_tokens.txt\"  # Full path to input tokenized file\n",
        "    model_directory = \"/content/drive/MyDrive/Korean_chat_summary/PEGASUS_tokenizer2\"\n",
        "    model_prefix = f\"{model_directory}/korean_bpe_unigram\"  # Specify the complete path for your model prefix\n",
        "    vocab_size = 96000  # Set your desired vocabulary size\n",
        "    special_tokens = [\"<cls>\", \"<sep>\", \"<unk>\", \"<pad>\", \"<mask_1>\", \"<mask_2>\", \"<s>\", \"</s>\"]\n",
        "\n",
        "    # Train the SentencePiece model\n",
        "    train_sentencepiece_model(input_file, model_prefix, vocab_size)\n",
        "\n",
        "    # Load the SentencePiece model\n",
        "    model_file = f\"{model_prefix}.model\"\n",
        "    sp = load_sentencepiece_model(model_file)\n",
        "\n",
        "    # Now you can use `sp` for tokenization\n",
        "    print(\"Tokenizer model files saved in the specified location.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "# Function to train the SentencePiece model\n",
        "def train_sentencepiece_model(input_file, model_prefix, vocab_size, special_tokens):\n",
        "    # Prepare special token options\n",
        "    special_token_string = ','.join(special_tokens)\n",
        "    # Train SentencePiece model\n",
        "    spm.SentencePieceTrainer.train(\n",
        "        f'--input={input_file} '\n",
        "        f'--model_prefix={model_prefix} '\n",
        "        f'--vocab_size={vocab_size} '\n",
        "        f'--model_type=unigram '\n",
        "        f'--unk_piece=\"<unk>\" '\n",
        "        f'--additional_vocab_size={len(special_tokens)} '\n",
        "        f'--unk_id=0'\n",
        "    )\n",
        "    print(f\"SentencePiece model training is completed. Model prefix: {model_prefix}\")\n",
        "\n",
        "    # Load the trained model for further processing\n",
        "    sp = spm.SentencePieceProcessor()\n",
        "    sp.load(f\"{model_prefix}.model\")\n",
        "\n",
        "    # Add special tokens manually to the model\n",
        "    for token in special_tokens:\n",
        "        sp.add_piece(token)\n",
        "\n",
        "    return sp\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_file = \"/content/drive/MyDrive/Korean_chat_summary/PEGASUS_okt_tokens.txt\"  # Full path to your input tokenized file\n",
        "    model_prefix = \"/content/drive/MyDrive/Korean_chat_summary/korean_unigram\"  # Prefix for your model and vocab files\n",
        "    vocab_size = 8000  # Set your desired vocabulary size\n",
        "    special_tokens = [\"<cls>\", \"<sep>\", \"<unk>\", \"<pad>\", \"<mask_1>\", \"<mask_2>\", \"<s>\", \"</s>\"]\n",
        "\n",
        "    # Train the SentencePiece model\n",
        "    sp = train_sentencepiece_model(input_file, model_prefix, vocab_size, special_tokens)\n",
        "\n",
        "    # Now you can use `sp` for tokenization\n",
        "    print(\"Special tokens were added to the SentencePiece model.\")\n"
      ],
      "metadata": {
        "id": "DUHCavbqxRXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hu8hssUMt4V-",
        "outputId": "b8323885-ab0c-4223-a66a-73f7acfffdda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Message: 토큰화할 문장입니다.\n",
            "Encoded IDs: tensor([[18108, 17715,  5804,  7415, 56051,  3115,     7]])\n",
            "Decoded Message: 토큰화할 문장입니다.\n",
            "\n",
            "Tokenized Encoding: ['▁토크', 'ᆫ화', '할', '▁문장', '입니다', '.', '</s>']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer_path = \"/content/drive/MyDrive/Korean_chat_summary/PEGASUS_tokenizer2\"  # Adjust your tokenizer path as needed\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
        "\n",
        "# Example Korean chat messages\n",
        "korean_chat_messages = [\"토큰화할 문장입니다.\"]\n",
        "\n",
        "\n",
        "# Function to encode and decode chat messages\n",
        "def process_chat_messages(messages):\n",
        "    for msg in messages:\n",
        "        # Encode the message\n",
        "        encoded_msg = tokenizer(msg, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "        # Print original message details\n",
        "        print(f\"Original Message: {msg}\")\n",
        "        print(f\"Encoded IDs: {encoded_msg['input_ids']}\")\n",
        "\n",
        "        # Decode the message back from tokens\n",
        "        decoded_msg = tokenizer.decode(encoded_msg['input_ids'][0], skip_special_tokens=True)\n",
        "        print(f\"Decoded Message: {decoded_msg}\\n\")\n",
        "\n",
        "        # Show tokenized encoding (mapping IDs to tokens)\n",
        "        tokens = tokenizer.convert_ids_to_tokens(encoded_msg['input_ids'][0].tolist())\n",
        "        print(f\"Tokenized Encoding: {tokens}\\n\")\n",
        "\n",
        "# Process the example Korean chat messages\n",
        "process_chat_messages(korean_chat_messages)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKDUE8nj-EyQ",
        "outputId": "fb2269a2-c062-4bd3-8777-c3cb9d80ae2d"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "로컬 SentencePiece 토크나이저 로드 완료\n",
            "100.00% 토큰화 완료\n",
            "train_dataset 토큰화 완료\n",
            "{'input_ids': [3801, 50376, 21325, 5321, 4991, 4747, 85, 188, 11930, 1389, 6796, 5494, 471, 1897, 1304, 6946, 1389, 1156, 1062, 17576, 6642, 5325, 1897, 410, 1972, 920, 14204, 1897, 410, 7878, 826, 3115, 2705, 3519, 2944, 62103, 3802, 50089, 44473, 14000, 36148, 16078, 188, 3519, 18236, 1304, 10365, 10676, 758, 16078, 541, 2046, 11375, 26553, 1326, 291, 1273, 10365, 541, 39844, 7278, 54136, 412, 1780, 6052, 7346, 188, 328, 410, 6066, 243, 799, 11, 13622, 291, 561, 48, 10400, 673, 216, 24, 3115, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [3801, 50376, 21325, 5321, 4991, 4747, 85, 188, 11930, 1389, 6796, 5494, 471, 1897, 1304, 6946, 1389, 1156, 1062, 17576, 6642, 5325, 1897, 410, 1972, 920, 14204, 1897, 410, 7878, 826, 3115, 2705, 3519, 2944, 62103, 3802, 50089, 44473, 14000, 36148, 16078, 188, 3519, 18236, 1304, 10365, 10676, 758, 16078, 541, 2046, 11375, 26553, 1326, 291, 1273, 10365, 541, 39844, 7278, 54136, 412, 1780, 6052, 7346, 188, 328, 410, 6066, 243, 799, 11, 13622, 291, 561, 48, 10400, 673, 216, 24, 3115, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]}\n"
          ]
        }
      ],
      "source": [
        "merged_data_path = \"/content/drive/MyDrive/Korean_chat_summary/tokenized_pretraining/kowikitext_split_0.txt\"\n",
        "\n",
        "# 파일 불러오기\n",
        "with open(merged_data_path, 'r', encoding='utf-8') as file:\n",
        "    train_dataset = file.read()\n",
        "\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer_path = \"/content/drive/MyDrive/Korean_chat_summary/PEGASUS_tokenizer2\"  # Adjust your tokenizer path as needed\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
        "\n",
        "print(\"로컬 SentencePiece 토크나이저 로드 완료\")\n",
        "\n",
        "def tokenize_data(data): # 수정 1: 토큰화 과정에서 진행상황 로깅할 수 있게 한 코드\n",
        "    # 데이터가 큰 경우 반복하면서 토큰화 진행\n",
        "    total_data_size = len(data)\n",
        "    tokenized_data = []\n",
        "\n",
        "    for idx, sample in enumerate(data):\n",
        "        # 입력 텍스트를 토큰화\n",
        "        inputs = tokenizer(\n",
        "            sample,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=128\n",
        "        )\n",
        "        # labels를 입력과 동일하게 설정\n",
        "        inputs['labels'] = inputs['input_ids'].copy()\n",
        "\n",
        "        tokenized_data.append(inputs)\n",
        "\n",
        "        # 진행 상황 출력\n",
        "        if (idx + 1) % 1000000 == 0 or (idx + 1) == total_data_size:  # 1000000개마다 출력 또는 마지막에 출력\n",
        "            percent_complete = (idx + 1) / total_data_size * 100\n",
        "            print(f\"{percent_complete:.2f}% 토큰화 완료\")\n",
        "\n",
        "\n",
        "    return tokenized_data\n",
        "\n",
        "# 데이터셋이 리스트 형태라고 가정\n",
        "train_dataset = train_dataset.splitlines()  # txt 파일 데이터를 리스트로 분할 ##수정2 : 로깅 위해 분할\n",
        "\n",
        "# 토큰화 진행\n",
        "tokenized_data = tokenize_data(train_dataset) ## 수정3 : 맵핑 없이 txt 데이터 바로 사용\n",
        "# 2019_1q만 토큰화 하는데도 약 100분정도 걸림 -> 전체 kcbert data 토큰화 하려면 약 100분*15=1500분=약 25시간 소요 예상\n",
        "# 한번 토큰화 후 토큰화 결과를 파일로 저장하고 아래 코드에서는 저장한 파일을 불러와 사용해야 함\n",
        "\n",
        "print(\"train_dataset 토큰화 완료\")\n",
        "print(tokenized_data[0])\n",
        "print(tokenized_data)\n",
        "\n",
        "# 토큰화된 데이터를 두가지 파일로 저장\n",
        "# 절대 경로로 파일 저장\n",
        "torch.save(tokenized_data, '/content/drive/MyDrive/Korean_chat_summary/tokenized_pretraining/PEGASUS_tokenized_data.pt') # 불러오기 : tokenized_data = torch.load('tokenized_data.pt')\n",
        "print(\"토큰화 결과 저장 완료\")\n",
        "\n",
        "# # 잘 저장되었는지 확인\n",
        "# tokenized_data = torch.load(f'/home/suyeon1803/PEGASUS_Pre/data/tokenized_data_45/merged_2019_1q_part_1.pt') # 2021_1q 데이터 확인해보기\n",
        "# print(tokenized_data)\n",
        "# print(len(tokenized_data))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer_path = \"/content/drive/MyDrive/Korean_chat_summary/PEGASUS_tokenizer2\"  # Adjust your tokenizer path as needed\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
        "\n",
        "def tokenize_data(data, save_path):  # Adding save_path to write data to file\n",
        "    total_data_size = len(data)\n",
        "    tokenized_data = []\n",
        "\n",
        "    for idx, sample in enumerate(data):\n",
        "        # Tokenize the input text\n",
        "        inputs = tokenizer(\n",
        "            sample,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=128\n",
        "        )\n",
        "        # Set labels identical to inputs\n",
        "        inputs['labels'] = inputs['input_ids'].copy()\n",
        "\n",
        "        # Append current inputs to tokenized_data\n",
        "        tokenized_data.append(inputs)\n",
        "\n",
        "        # Save to file incrementally\n",
        "        if (idx + 1) % 10000 == 0:  # Write every 10000 samples or as appropriate\n",
        "            torch.save(tokenized_data, save_path)\n",
        "            tokenized_data = []  # Clear the current data after saving to free memory\n",
        "\n",
        "        # Log progress\n",
        "        if (idx + 1) % 1000000 == 0 or (idx + 1) == total_data_size:\n",
        "            percent_complete = (idx + 1) / total_data_size * 100\n",
        "            print(f\"{percent_complete:.2f}% 토큰화 완료\")\n",
        "\n",
        "    # Final save after loop ends\n",
        "    if tokenized_data:\n",
        "        torch.save(tokenized_data, save_path)\n",
        "\n",
        "    return save_path\n",
        "\n",
        "# Usage:\n",
        "merged_data_path = \"/content/drive/MyDrive/Korean_chat_summary/tokenized_pretraining/kowikitext_split_0.txt\"\n",
        "\n",
        "# 파일 불러오기\n",
        "with open(merged_data_path, 'r', encoding='utf-8') as file:\n",
        "    train_dataset = file.read()\n",
        "\n",
        "tokenized_data_path = '/content/drive/MyDrive/Korean_chat_summary/tokenized_pretraining/PEGASUS_tokenized_data.pt'\n",
        "tokenize_data(train_dataset, tokenized_data_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "3uGfuo2aE3ia",
        "outputId": "ea63547f-3640-454a-c87a-f661f9e17da9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.81% 토큰화 완료\n",
            "9.62% 토큰화 완료\n",
            "14.43% 토큰화 완료\n",
            "19.24% 토큰화 완료\n",
            "24.05% 토큰화 완료\n",
            "28.86% 토큰화 완료\n",
            "33.67% 토큰화 완료\n",
            "38.48% 토큰화 완료\n",
            "43.29% 토큰화 완료\n",
            "48.10% 토큰화 완료\n",
            "52.91% 토큰화 완료\n",
            "57.72% 토큰화 완료\n",
            "62.53% 토큰화 완료\n",
            "67.34% 토큰화 완료\n",
            "72.15% 토큰화 완료\n",
            "76.96% 토큰화 완료\n",
            "81.78% 토큰화 완료\n",
            "86.59% 토큰화 완료\n",
            "91.40% 토큰화 완료\n",
            "96.21% 토큰화 완료\n",
            "100.00% 토큰화 완료\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Korean_chat_summary/tokenized_pretraining/PEGASUS_tokenized_data.pt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze | grep transformers\n",
        "!pip freeze | grep torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKszGXpm7zEz",
        "outputId": "399a39b1-37c5-456f-f380-ce799ad76ec0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transformers==4.44.2\n",
            "torch @ https://download.pytorch.org/whl/cu121_full/torch-2.4.1%2Bcu121-cp310-cp310-linux_x86_64.whl#sha256=f3ed9a2b7f8671b2b32a2f036d1b81055eb3ad9b18ba43b705aa34bae4289e1a\n",
            "torchaudio @ https://download.pytorch.org/whl/cu121_full/torchaudio-2.4.1%2Bcu121-cp310-cp310-linux_x86_64.whl#sha256=da8c87c80a1c1376a48dc33eef30b03bbdf1df25a05bd2b1c620b8811c7b19be\n",
            "torchsummary==1.5.1\n",
            "torchvision @ https://download.pytorch.org/whl/cu121_full/torchvision-0.19.1%2Bcu121-cp310-cp310-linux_x86_64.whl#sha256=b8cc4bf381b75522995b601e07a1b433b5fd925dc3e34a7fa6cd22f449d65379\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}